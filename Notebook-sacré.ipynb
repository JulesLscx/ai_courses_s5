{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23ce6b26e885007a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-22T21:17:23.483349300Z",
     "start_time": "2024-01-22T21:17:16.328812500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fonctions utiles pour récupérer la data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3759576856319c9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_data(n):\n",
    "    data = pd.read_csv('../data/train.new.csv')\n",
    "    return data[0:n]\n",
    "\n",
    "def select_variables(data):\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "    y = data['smoking'] # récupérer la colonne survived et la mettre dans y\n",
    "    X = data.drop('country', axis=1)\n",
    "    X = X.drop('id', axis=1)\n",
    "    X = X.replace([\"M\", \"F\"], [0, 1])\n",
    "    X = X.drop('smoking', axis=1)\n",
    "    return X,y\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51570fa309eaf5b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exemple de code pour récupérer la data et la préparer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4518ee811ec35f51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "normalizer = RobustScaler() # normalise les données\n",
    "data=load_data(-1)\n",
    "# sélectionner les variables\n",
    "X,y = select_variables(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4f2b189882fef78"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Création du modèle"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16e2b54f7a9b1d7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Add layers based on the best hyperparameters\n",
    "    for _ in range(hp.Int('nb_layers', min_value=2, max_value=5)):\n",
    "        if model.layers != []:\n",
    "            if hp.Choice('add_dropout_and_batch', values=[True, False]):\n",
    "                model.add(layers.Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "                model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dense(units=hp.Int('nb_units', min_value=32, max_value=528, step=64),\n",
    "                               activation='relu'))\n",
    "        model.add(layers.C)\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # Decay learning rate\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]),\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_hierarchical_model(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Add layers based on the best hyperparameters\n",
    "    tot_units = hp.Int('nb_units', min_value=32, max_value=528, step=64)\n",
    "    #each layer have 2 times less units than the previous one\n",
    "    while tot_units >= 32:\n",
    "        if (model.layers != []):\n",
    "            model.add(layers.Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "            model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dense(units=tot_units, activation='relu'))\n",
    "        tot_units = tot_units // 2\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def tune_model(X_train, y_train, X_test, y_test):\n",
    "    tuner = Hyperband(build_hierarchical_model,\n",
    "                      objective='val_accuracy',\n",
    "                      max_epochs=30,\n",
    "                      factor=3,\n",
    "                      hyperband_iterations=8,\n",
    "                      directory='my_dir',\n",
    "                      project_name='intro_to_kt')\n",
    "\n",
    "    tuner.search(X_train, y_train,\n",
    "                 epochs=50,\n",
    "                 validation_data=(X_test, y_test))\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    return best_hps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8faa9e032fd535a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Récupération des meilleurs hyperparamètres"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57b88662887a7935"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_hps = tune_model(X_train, y_train, X_test, y_test)\n",
    "best_hps.get('learning_rate'), best_hps.get('nb_units'), best_hps.get('dropout')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3302db477eddf8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# # Meilleur modèle pour l'instant :\n",
    "- 3 couches cachées\n",
    "- Une de 96 neurones\n",
    "- Une de 48 neurones\n",
    "- Une de 24 neurones\n",
    "- Dropout de 0.2\n",
    "- Batch Normalisation avant l'activation\n",
    "- Optimiseur : Adam\n",
    "- Learning rate : 0.001 (test en cours avec un learning rate dynamique)\n",
    "\n",
    "# Résultats :\n",
    "- 0.777 de précision sur le test set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deaaa70f313d0ddf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Créer son modèle défini si demandé dans le sujet\n",
    "# Layers input -> X_train.shape[1] (nombre de colonnes)\n",
    "# Dense -> nombre de neurones\n",
    "# Dropout -> pourcentage de neurones à désactiver pour éviter l'overfitting\n",
    "# BatchNormalization -> normalise les données pour éviter l'overfitting\n",
    "# Dense -> 1 neurone pour la sortie\n",
    "# Optimizer -> Adam (le plus utilisé)\n",
    "# lr -> learning rate (taux d'apprentissage)\n",
    "# loss -> binary_crossentropy (car problème de classification binaire)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6d3e0f2ba878f22"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(units=526, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(units=128, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(units=32, activation='relu'))\n",
    "model.add(layers.Dropout(best_hps.get('dropout')))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "lr = best_hps.get('learning_rate')\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), batch_size=16)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "703ba279ffae387"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Affichage des résultats"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f288033549317ed3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend(['Train', 'Test'], loc='right')\n",
    "plt.title('Model accuracy with best hyperparameters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10a3d63003cdf438"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Matrice de confusion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb75f04503ae91dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def visualiser_confusion(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Get class labels\n",
    "    y_classes = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_classes)\n",
    "    #disp= ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "    sns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
    "    plt.show()\n",
    "visualiser_confusion(model, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2850b23e33017150"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation du modèle"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c45a1fdf75112ae6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94dd9d23892f926f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prédiction sur le test set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75b15047b7af8def"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.new.csv')\n",
    "test = test.drop('country', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "test = test.replace([\"M\", \"F\"], [0, 1])\n",
    "test = normalizer.transform(test)\n",
    "y_pred = model.predict(test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred.to_csv('../data/y_pred.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1658b8e366466eeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sauvegarde du modèle"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a080753891ca87e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.save('../models/model.h5')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a78ca804029836b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chargement du modèle"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebc8a2422fb38754"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../models/model.h5')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee0cb7a8898295f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sauvegarde de l'historique"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56be9a55ee727c91"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_csv_file = '../models/history.csv'\n",
    "hist_df.to_csv(hist_csv_file, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d74b271f0a16a142"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fonctions d'activation pour la classification\n",
    "- sigmoid : 0 ou 1 pour la classification binaire\n",
    "- softmax : probabilité pour chaque classe pour la classification multiclasse\n",
    "- relu : 0 ou 1 pour la classification binaire\n",
    "- mish : -1 ou 1 pour la classification binaire couche cachée\n",
    "- exponential : 0 à +inf pour la régression\n",
    "- linear : -inf à +inf pour la régression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb88cd3a9fa4cbb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Préprocessing à utiliser sur les données\n",
    "- Gestion des collonnes textuelles -> LabelEncoder (pour les variables catégorielles), ou alors drop les colonnes\n",
    "- Gestion des valeurs manquantes -> drop les lignes ou remplacer par la moyenne ou la médiane\n",
    "- Normalisation -> StandardScaler ou MinMaxScaler ou RobustScaler\n",
    "- Note : Bien séparer les données en train et test avant de faire le preprocessing et s'assurer que les données de test ne sont pas utilisées pour le preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a62189fff26309"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Charger les données\n",
    "data = load_data(n)\n",
    "\n",
    "data.dropna(axis=0, inplace=True) # supprimer les lignes avec des valeurs manquantes\n",
    "\n",
    "# Sélectionner les variables\n",
    "X, y = select_variables(data)\n",
    "\n",
    "# Gérer les colonnes textuelles\n",
    "label_encoder = LabelEncoder()\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "# Séparer les données en train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "\n",
    "# Gérer les valeurs manquantes si je n'utilise pas le dropna\n",
    "imputer = SimpleImputer(strategy='mean')  # ou 'median' pour la médiane\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Normaliser les données\n",
    "normalizer = RobustScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31ea08a21064cdd4"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aeee17467ced2dbd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
